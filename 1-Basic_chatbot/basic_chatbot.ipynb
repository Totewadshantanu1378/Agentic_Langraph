{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90300330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import START , END , StateGraph\n",
    "from langgraph.graph.message import add_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5caaa6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    # messages have the type \"list\" . add_messages function in \n",
    "    # the anotation defines how the state key should be updated \n",
    "    # in this case it appends the list instead of overwriting\n",
    "    messages : Annotated[list[str] , add_messages]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727813d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d356ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = ChatGroq(model = \"llama3-8b-8192\")\n",
    "\n",
    "# you have to initilize the model name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20215153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000021C36A7CEC0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000021C36A7DA90>, model_name='llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "177fac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state : State):\n",
    "    return {\"messages\":[llm.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30cd7df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# adding the node \n",
    "graph_builder.add_node(\"llmchatbot\" , chatbot)\n",
    "# adding the edges \n",
    "graph_builder.add_edge(START , \"llmchatbot\")\n",
    "graph_builder.add_edge(\"llmchatbot\" , END)\n",
    "\n",
    "# compile the graph \n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b7922f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"messages\" : \"Hi\"})\n",
    "\n",
    "# this message will be appeneded in the State defined using annotated \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaadf604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0163fa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I'm just an AI, so I don't have feelings like humans do, but I'm functioning properly and ready to help with any questions or tasks you may have! How about you? How's your day going?\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream({\"messages\" : \"hi How are you?\"}):\n",
    "    for ev in event.values():\n",
    "        print(ev[\"messages\"][-1].content)\n",
    "    \n",
    "# it shows that whenever you do the graphstream it will only give you the ai messgaes not the humans \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636188f7",
   "metadata": {},
   "source": [
    "### chatbot using Tools Tavily "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab8ea2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'who current prime minister of india',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'url': 'https://en.wikipedia.org/wiki/Prime_Minister_of_India',\n",
       "   'title': 'Prime Minister of India - Wikipedia',\n",
       "   'content': 'Modi is the current prime minister of India, serving since 26 May 2014 and the first to win three consecutive elections to secure a third successive term, 2014,',\n",
       "   'score': 0.9016528,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.pmindia.gov.in/en/pms-profile/',\n",
       "   'title': 'Know the PM | Prime Minister of India',\n",
       "   'content': \"Shri Narendra Modi was sworn-in as India's Prime Minister for the third time on 9th June 2024, following another decisive victory in the 2024 Parliamentary\",\n",
       "   'score': 0.84858394,\n",
       "   'raw_content': None},\n",
       "  {'url': 'http://www.pmindia.gov.in/en/',\n",
       "   'title': 'Prime Minister of India',\n",
       "   'content': \"Shri Narendra Modi was sworn-in as India's Prime Minister for the third time on 9th June 2024, following another decisive victory in the 2024 Parliamentary\",\n",
       "   'score': 0.8315952,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://en.wikipedia.org/wiki/Narendra_Modi',\n",
       "   'title': 'Narendra Modi - Wikipedia',\n",
       "   'content': 'Narendra Damodardas Modi (born 17 September 1950) is an Indian politician who has served as the prime minister of India since 2014.',\n",
       "   'score': 0.80735475,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.britannica.com/biography/Narendra-Modi',\n",
       "   'title': 'Narendra Modi | Biography, Career, & Facts - Britannica',\n",
       "   'content': 'Narendra Modi is the 14th prime minister of India. His Hindu nationalist policies and some of his economic reforms have proved controversial to many within',\n",
       "   'score': 0.6778372,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 1.4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "tool = TavilySearch(max_result = 2)\n",
    "tool.invoke(\"who current prime minister of india\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f2785db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a:int , b:int)->int:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    return a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c25f00c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [tool , multiply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c438b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmwithtools = llm.bind_tools(tools)\n",
    "\n",
    "# it will bind llm with tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a97ef99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000021C36A7CEC0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000021C36A7DA90>, model_name='llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'tavily_search', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. It not only retrieves URLs and snippets, but offers advanced search depths, domain management, time range filters, and image search, this tool delivers real-time, accurate, and citation-backed results.Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'Search query to look up', 'type': 'string'}, 'include_domains': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'}, {'type': 'null'}], 'default': [], 'description': 'A list of domains to restrict search results to.\\n\\n        Use this parameter when:\\n        1. The user explicitly requests information from specific websites (e.g., \"Find climate data from nasa.gov\")\\n        2. The user mentions an organization or company without specifying the domain (e.g., \"Find information about iPhones from Apple\")\\n\\n        In both cases, you should determine the appropriate domains (e.g., [\"nasa.gov\"] or [\"apple.com\"]) and set this parameter.\\n\\n        Results will ONLY come from the specified domains - no other sources will be included.\\n        Default is None (no domain restriction).\\n        '}, 'exclude_domains': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'}, {'type': 'null'}], 'default': [], 'description': 'A list of domains to exclude from search results.\\n\\n        Use this parameter when:\\n        1. The user explicitly requests to avoid certain websites (e.g., \"Find information about climate change but not from twitter.com\")\\n        2. The user mentions not wanting results from specific organizations without naming the domain (e.g., \"Find phone reviews but nothing from Apple\")\\n\\n        In both cases, you should determine the appropriate domains to exclude (e.g., [\"twitter.com\"] or [\"apple.com\"]) and set this parameter.\\n\\n        Results will filter out all content from the specified domains.\\n        Default is None (no domain exclusion).\\n        '}, 'search_depth': {'anyOf': [{'enum': ['basic', 'advanced'], 'type': 'string'}, {'type': 'null'}], 'default': 'basic', 'description': 'Controls search thoroughness and result comprehensiveness.\\n    \\n        Use \"basic\" for simple queries requiring quick, straightforward answers.\\n        \\n        Use \"advanced\" (default) for complex queries, specialized topics, \\n        rare information, or when in-depth analysis is needed.\\n        '}, 'include_images': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}], 'default': False, 'description': 'Determines if the search returns relevant images along with text results.\\n   \\n        Set to True when the user explicitly requests visuals or when images would \\n        significantly enhance understanding (e.g., \"Show me what black holes look like,\" \\n        \"Find pictures of Renaissance art\").\\n        \\n        Leave as False (default) for most informational queries where text is sufficient.\\n        '}, 'time_range': {'anyOf': [{'enum': ['day', 'week', 'month', 'year'], 'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Limits results to content published within a specific timeframe.\\n        \\n        ONLY set this when the user explicitly mentions a time period \\n        (e.g., \"latest AI news,\" \"articles from last week\").\\n        \\n        For less popular or niche topics, use broader time ranges \\n        (\"month\" or \"year\") to ensure sufficient relevant results.\\n   \\n        Options: \"day\" (24h), \"week\" (7d), \"month\" (30d), \"year\" (365d).\\n        \\n        Default is None.\\n        '}, 'topic': {'anyOf': [{'enum': ['general', 'news', 'finance'], 'type': 'string'}, {'type': 'null'}], 'default': 'general', 'description': 'Specifies search category for optimized results.\\n   \\n        Use \"general\" (default) for most queries, INCLUDING those with terms like \\n        \"latest,\" \"newest,\" or \"recent\" when referring to general information.\\n\\n        Use \"finance\" for markets, investments, economic data, or financial news.\\n\\n        Use \"news\" ONLY for politics, sports, or major current events covered by \\n        mainstream media - NOT simply because a query asks for \"new\" information.\\n        '}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'multiply', 'description': '', 'parameters': {'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}}, 'required': ['a', 'b'], 'type': 'object'}}}]}, config={}, config_factories=[])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmwithtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0253f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START ,END , StateGraph\n",
    "from langgraph.prebuilt import ToolNode ,tools_condition\n",
    "# it is used for cheking which tool is used by llm \n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "def tool_calling_llm(state:State):\n",
    "    return {\"messages\":[llmwithtools.invoke(state['messages'])]}\n",
    "\n",
    "# graph \n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"tool_calling_llm\",tool_calling_llm)\n",
    "builder.add_node(\"tools\" , ToolNode(tools))\n",
    "\n",
    "builder.add_edge(START , \"tool_calling_llm\")\n",
    "builder.add_conditional_edges(\n",
    "    \"tool_calling_llm\",\n",
    "    # if the latest message from assistant is tool call -> tool conditon routes to tools \n",
    "    # if the latest message from assistant is not a tool call -> tool condition routes to END \n",
    "    # that is what tools_condition does \n",
    "    tools_condition\n",
    ")\n",
    "\n",
    "builder.add_edge(\"tools\" , END)    \n",
    "# you can make it react agent architecture as well   END->tool_calling_llm\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23b2743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"messages\" : \"give cpp code for prime number\"})\n",
    "\n",
    "# here you caan see , there is no ai message , it does a tool call and you can see the tool message "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca24ce67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "give cpp code for prime number\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here is an example of C++ code that checks if a number is prime:\n",
      "```\n",
      "#include <iostream>\n",
      "\n",
      "bool isPrime(int n) {\n",
      "  if (n <= 1) return false; // 0 and 1 are not prime\n",
      "  for (int i = 2; i * i <= n; i++) {\n",
      "    if (n % i == 0) return false; // if n is divisible by i, it's not prime\n",
      "  }\n",
      "  return true; // if n is not divisible by any i, it's prime\n",
      "}\n",
      "\n",
      "int main() {\n",
      "  int num;\n",
      "  std::cout << \"Enter a number: \";\n",
      "  std::cin >> num;\n",
      "  if (isPrime(num)) {\n",
      "    std::cout << num << \" is a prime number.\" << std::endl;\n",
      "  } else {\n",
      "    std::cout << num << \" is not a prime number.\" << std::endl;\n",
      "  }\n",
      "  return 0;\n",
      "}\n",
      "```\n",
      "This code uses a simple algorithm to check if a number is prime. It works by iterating from 2 to the square root of the number and checking if the number is divisible by any of the values in that range. If it is, then the number is not prime. If it is not divisible by any of the values in that range, then it is prime.\n",
      "\n",
      "You can also use a more optimized algorithm to check for primality, such as the Miller-Rabin primality test. Here is an example of how you could implement that in C++:\n",
      "```\n",
      "#include <iostream>\n",
      "#include <random>\n",
      "\n",
      "bool isPrime(int n, int k = 5) {\n",
      "  if (n <= 1) return false; // 0 and 1 are not prime\n",
      "  if (n == 2 || n == 3) return true; // 2 and 3 are prime\n",
      "  if (n % 2 == 0) return false; // even numbers greater than 2 are not prime\n",
      "\n",
      "  // Perform k iterations of the Miller-Rabin test\n",
      "  for (int i = 0; i < k; i++) {\n",
      "    int a = rand() % (n - 1) + 1;\n",
      "    int x = powerMod(a, (n - 1) / 2, n);\n",
      "    if (x == 1 || x == n - 1) continue;\n",
      "    for (int j = 0; j < k - 1; j++) {\n",
      "      x = powerMod(x, 2, n);\n",
      "      if (x == n - 1) break;\n",
      "    }\n",
      "    if (x != n - 1) return false;\n",
      "  }\n",
      "  return true;\n",
      "}\n",
      "\n",
      "int powerMod(int base, int exponent, int mod) {\n",
      "  int result = 1;\n",
      "  while (exponent > 0) {\n",
      "    if (exponent % 2 == 1) result = (result * base) % mod;\n",
      "    base = (base * base) % mod;\n",
      "    exponent /= 2;\n",
      "  }\n",
      "  return result;\n",
      "}\n",
      "\n",
      "int main() {\n",
      "  int num;\n",
      "  std::cout << \"Enter a number: \";\n",
      "  std::cin >> num;\n",
      "  if (isPrime(num)) {\n",
      "    std::cout << num << \" is a prime number.\" << std::endl;\n",
      "  } else {\n",
      "    std::cout << num << \" is not a prime number.\" << std::endl;\n",
      "  }\n",
      "  return 0;\n",
      "}\n",
      "```\n",
      "This code uses the Miller-Rabin primality test to check if a number is prime. The test works by repeatedly testing if the number is a witness to the compositeness of the number. If it finds a witness, it returns false; otherwise, it returns true. The test is repeated k times to ensure that the result is correct.\n",
      "\n",
      "Note that the Miller-Rabin test is not a deterministic test, and there is a small chance that it may return a false positive (i.e., a composite number that is reported as prime). However, the probability of this occurring is very small, and the test is generally considered to be reliable.\n"
     ]
    }
   ],
   "source": [
    "for n in response['messages']:\n",
    "    n.pretty_print()\n",
    "    \n",
    "# here you can see the llm made multiply tool call "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f54af62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"messages\" : \"multiply 2178 by 212 and also give me the recent ai news\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d8942ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "multiply 2178 by 212 and also give me the recent ai news\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (2sbmybv4t)\n",
      " Call ID: 2sbmybv4t\n",
      "  Args:\n",
      "    a: 2178\n",
      "    b: 212\n",
      "  tavily_search (mey2pt6w6)\n",
      " Call ID: mey2pt6w6\n",
      "  Args:\n",
      "    query: recent ai news\n",
      "    search_depth: advanced\n",
      "    topic: general\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "461736\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{\"query\": \"recent ai news\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.crescendo.ai/news/latest-ai-news-and-updates\", \"title\": \"Latest AI Breakthroughs and News: May, June, July 2025\", \"content\": \"New AI Model Predicts Human Decisions with Surprising Precision . Summary: Scientists have developed an AI model that mimics human decision-making with high accuracy in complex moral and social dilemmas. The system integrates cognitive science with deep learning to simulate how people weigh trade-offs.\", \"score\": 0.77446437, \"raw_content\": null}, {\"url\": \"https://www.artificialintelligence-news.com/\", \"title\": \"AI News | Latest AI News, Analysis & Events\", \"content\": \"SoftBank chief: Forget AGI, ASI will be here within 10 years ; Salesforce to buy Informatica in $8B deal · Artificial Intelligence. May 28, 2025 ; Huawei\", \"score\": 0.7065353, \"raw_content\": null}, {\"url\": \"https://blog.google/technology/ai/google-ai-updates-june-2025/\", \"title\": \"The latest AI news we announced in June - Google Blog\", \"content\": \"Here's a recap of some of our biggest AI updates from June, including more ways to search with AI Mode, a new way to share your NotebookLM\", \"score\": 0.6461259, \"raw_content\": null}, {\"url\": \"https://news.un.org/en/story/2025/07/1165346\", \"title\": \"UN summit confronts AI's dawn of wonders and warnings - UN News\", \"content\": \"Her remarks reflected a growing sense of urgency among policymakers and technologists, as new “agentic AI” systems capable of autonomous reasoning and action emerge at unprecedented speed.\\n\\nWith some experts predicting human-level AI within the next three years, concerns about safety, bias, energy consumption and regulatory capacity have intensified.\\n\\nTech on display at the AI for Good Global Summit.\\n\\n## Tech on display\\n\\nThe summit’s agenda reflects these tensions. [...] One highlight will be the AI Governance Day on Thursday, where national regulators and international organizations will address the gap in global oversight. An ITU survey found that 85 per cent of countries lack an AI-specific policy or strategy, raising alarms about uneven development and growing digital divides.\\n\\n## Focus on health\\n\\nHealth is a prominent theme this year. [...] The AI for Good Global Summit 2025 brings together governments, tech leaders, academics, civil society and young people to explore how artificial intelligence can be directed toward Sustainable Development Goals (SDGs) – and away from growing risks of inequality, disinformation and environmental strain.\", \"score\": 0.57837737, \"raw_content\": null}, {\"url\": \"https://www.morganstanley.com/insights/articles/ai-trends-reasoning-frontier-models-2025-tmt\", \"title\": \"5 AI Trends Shaping Innovation and ROI in 2025 | Morgan Stanley\", \"content\": \"The top trends in new AI frontiers and the focus on enterprises include AI reasoning, custom silicon, cloud migrations, systems to measure AI efficacy and building an agentic AI future .\", \"score\": 0.4613328, \"raw_content\": null}], \"response_time\": 6.69}\n"
     ]
    }
   ],
   "source": [
    "for n in response['messages']:\n",
    "    n.pretty_print()\n",
    "    \n",
    "# bith of the tools calls are made \n",
    "\n",
    "\n",
    "# there is another concept of react agent arcchitecture in which after \n",
    "# you tool call it doenot ends , it again go to llm, and perfrom the cycle\n",
    "# untill the answer is totally satisfied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc362a",
   "metadata": {},
   "source": [
    "### Adding memory for a session of Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33942d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use memory saver funciton of langgraph to save the previous interaction \n",
    "# and we have to add this memory while compiling a graaph\n",
    "\n",
    "from langgraph.graph import START ,END , StateGraph\n",
    "from langgraph.prebuilt import ToolNode ,tools_condition\n",
    "# it is used for cheking which tool is used by llm \n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "# memory saver function \n",
    "\n",
    "def tool_calling_llm(state:State):\n",
    "    return {\"messages\":[llmwithtools.invoke(state['messages'])]}\n",
    "\n",
    "# graph \n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"tool_calling_llm\",tool_calling_llm)\n",
    "builder.add_node(\"tools\" , ToolNode(tools))\n",
    "\n",
    "builder.add_edge(START , \"tool_calling_llm\")\n",
    "builder.add_conditional_edges(\n",
    "    \"tool_calling_llm\",\n",
    "    # if the latest message from assistant is tool call -> tool conditon routes to tools \n",
    "    # if the latest message from assistant is not a tool call -> tool condition routes to END \n",
    "    # that is what tools_condition does \n",
    "    tools_condition\n",
    ")\n",
    "\n",
    "builder.add_edge(\"tools\" , END)    \n",
    "# you can make it react agent architecture as well   END->tool_calling_llm\n",
    "\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30a74224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Shantanu! How can I assist you today?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have to first configure the session by giving the thread id \n",
    "# so that the model should store the data in memory for that perticular session with id \n",
    "\n",
    "config = {\"configurable\":{\"thread_id\" : \"1\"}}  \n",
    "# thread id should be unique \n",
    "\n",
    "# again invoke the graph \n",
    "response = graph.invoke({\"messages\" : \"hi my name is shantanu\"} , config=config)\n",
    "\n",
    "# we have to also tell the model for which thread we are providing the message, i.e. ->  config = config\n",
    "response['messages'][-1].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc149c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'ve been trained on various natural language processing techniques, including machine learning algorithms, to recognize and remember names. When you introduced yourself as \"Shantanu\", I stored that information in my memory so that I can recall it later.\\n\\nAs for your good name, I think it\\'s a lovely question! In many cultures, people are referred to by various names, such as a given name, surname, nickname, or even a title. Could you please elaborate on what you mean by \"good name\"? Are you referring to a specific cultural or spiritual context, or is it a personal question?\\n\\nFeel free to share more, and I\\'ll do my best to understand and respond thoughtfully!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = graph.invoke({\"messages\" : \"hi what is my good name ? and how do you remember my name ?\"} , config=config)\n",
    "\n",
    "# now it will give my name because previously i have given a prompt of my name , and model saved it in this session\n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4b31f6",
   "metadata": {},
   "source": [
    "### Streaming in Langgraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09bd7e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory= MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc41bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def superbot(state : State):\n",
    "    return {\"messages\" : [llm.invoke(state['messages'])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7af9051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph \n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"superbot\" , superbot)\n",
    "graph.add_edge(START , \"superbot\")\n",
    "graph.add_edge(\"superbot\" , END)\n",
    "\n",
    "graph_builder = graph.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ee453b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='hi my name is shantanu , i like football', additional_kwargs={}, response_metadata={}, id='2fe4bcf4-6cb5-4b32-84b4-510c2a681cc4')]}\n",
      "{'messages': [HumanMessage(content='hi my name is shantanu , i like football', additional_kwargs={}, response_metadata={}, id='2fe4bcf4-6cb5-4b32-84b4-510c2a681cc4'), AIMessage(content='Hi Shantanu! Nice to meet you! Football, eh? Which team do you support?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 21, 'total_tokens': 42, 'completion_time': 0.014808489, 'prompt_time': 0.00304615, 'queue_time': 0.287159621, 'total_time': 0.017854639}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_8b7c3a83f7', 'finish_reason': 'stop', 'logprobs': None}, id='run--110aab90-f825-439a-b5b1-29832642ba70-0', usage_metadata={'input_tokens': 21, 'output_tokens': 21, 'total_tokens': 42})]}\n"
     ]
    }
   ],
   "source": [
    "#there are some streaming methods called as stream , astream.\n",
    "# addtional parameters are values and updates \n",
    "# values = You get only the final output(s) of the graph — i.e., the end result(s) of your flow \n",
    "# updates = You get detailed events — each time a node runs, edges are traversed, and state is updated.\n",
    "\n",
    "\n",
    "config = {\"configurable\":{\"thread_id\" : \"1\"}}  \n",
    "# thread id should be unique \n",
    "\n",
    "# again invoke the graph \n",
    "for chunk in graph_builder.stream({\"messages\" : \"hi my name is shantanu , i like football\"} , config , stream_mode=\"values\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b88a6872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'superbot': {'messages': [AIMessage(content=\"Hello Shantanu! Nice to meet you! Tennis is a great sport, isn't it? Who's your favorite tennis player? Roger Federer, Rafael Nadal, or maybe Novak Djokovic?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 22, 'total_tokens': 66, 'completion_time': 0.069668569, 'prompt_time': 0.005328489, 'queue_time': 0.28183841, 'total_time': 0.074997058}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_8dc6ecaf8e', 'finish_reason': 'stop', 'logprobs': None}, id='run--324ca43b-e2a1-4869-8174-8adcbc829fa9-0', usage_metadata={'input_tokens': 22, 'output_tokens': 44, 'total_tokens': 66})]}}\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\":{\"thread_id\" : \"2\"}}  \n",
    "\n",
    "\n",
    "for chunk in graph_builder.stream({\"messages\" : \"hi my name is shantanu , i also like tennis\"} , config , stream_mode=\"updates\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106b607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agentic_Langraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
